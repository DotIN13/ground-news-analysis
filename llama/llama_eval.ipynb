{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71fc95b6-f1e0-408b-bd88-36a3989f05ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50958e17-728a-43a6-89a7-9a096753776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_distill_logits import TextClassificationDataset, create_collate_fn, load_datasets, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee0aed7-17a7-4554-8afd-1fb83a503940",
   "metadata": {},
   "source": [
    "# MIT LlaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1ebd669-8151-4985-b886-015f5d0c753f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda:0\n",
      "The model 'PeftModelForSequenceClassification' is not supported for text-classification. Supported models are ['AlbertForSequenceClassification', 'BartForSequenceClassification', 'BertForSequenceClassification', 'BigBirdForSequenceClassification', 'BigBirdPegasusForSequenceClassification', 'BioGptForSequenceClassification', 'BloomForSequenceClassification', 'CamembertForSequenceClassification', 'CanineForSequenceClassification', 'LlamaForSequenceClassification', 'ConvBertForSequenceClassification', 'CTRLForSequenceClassification', 'Data2VecTextForSequenceClassification', 'DebertaForSequenceClassification', 'DebertaV2ForSequenceClassification', 'DiffLlamaForSequenceClassification', 'DistilBertForSequenceClassification', 'ElectraForSequenceClassification', 'ErnieForSequenceClassification', 'ErnieMForSequenceClassification', 'EsmForSequenceClassification', 'FalconForSequenceClassification', 'FlaubertForSequenceClassification', 'FNetForSequenceClassification', 'FunnelForSequenceClassification', 'GemmaForSequenceClassification', 'Gemma2ForSequenceClassification', 'GlmForSequenceClassification', 'GPT2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPTBigCodeForSequenceClassification', 'GPTNeoForSequenceClassification', 'GPTNeoXForSequenceClassification', 'GPTJForSequenceClassification', 'HeliumForSequenceClassification', 'IBertForSequenceClassification', 'JambaForSequenceClassification', 'JetMoeForSequenceClassification', 'LayoutLMForSequenceClassification', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv3ForSequenceClassification', 'LEDForSequenceClassification', 'LiltForSequenceClassification', 'LlamaForSequenceClassification', 'LongformerForSequenceClassification', 'LukeForSequenceClassification', 'MarkupLMForSequenceClassification', 'MBartForSequenceClassification', 'MegaForSequenceClassification', 'MegatronBertForSequenceClassification', 'MistralForSequenceClassification', 'MixtralForSequenceClassification', 'MobileBertForSequenceClassification', 'ModernBertForSequenceClassification', 'MPNetForSequenceClassification', 'MptForSequenceClassification', 'MraForSequenceClassification', 'MT5ForSequenceClassification', 'MvpForSequenceClassification', 'NemotronForSequenceClassification', 'NezhaForSequenceClassification', 'NystromformerForSequenceClassification', 'OpenLlamaForSequenceClassification', 'OpenAIGPTForSequenceClassification', 'OPTForSequenceClassification', 'PerceiverForSequenceClassification', 'PersimmonForSequenceClassification', 'PhiForSequenceClassification', 'Phi3ForSequenceClassification', 'PhimoeForSequenceClassification', 'PLBartForSequenceClassification', 'QDQBertForSequenceClassification', 'Qwen2ForSequenceClassification', 'Qwen2MoeForSequenceClassification', 'ReformerForSequenceClassification', 'RemBertForSequenceClassification', 'RobertaForSequenceClassification', 'RobertaPreLayerNormForSequenceClassification', 'RoCBertForSequenceClassification', 'RoFormerForSequenceClassification', 'SqueezeBertForSequenceClassification', 'StableLmForSequenceClassification', 'Starcoder2ForSequenceClassification', 'T5ForSequenceClassification', 'TapasForSequenceClassification', 'TransfoXLForSequenceClassification', 'UMT5ForSequenceClassification', 'XLMForSequenceClassification', 'XLMRobertaForSequenceClassification', 'XLMRobertaXLForSequenceClassification', 'XLNetForSequenceClassification', 'XmodForSequenceClassification', 'YosoForSequenceClassification', 'ZambaForSequenceClassification', 'Zamba2ForSequenceClassification'].\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the *base* LLaMA model for sequence classification\n",
    "base_model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer_name = \"tokenizer\"\n",
    "access_token = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_name,\n",
    "    token=access_token,\n",
    "    num_labels=3,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# Set pad token\n",
    "tokenizer.add_special_tokens({\"pad_token\":\"[PAD]\"})\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 2. Load the LoRA adapter on top of the base model\n",
    "adapter_path = \"best_model/best_model_r16_b4\"\n",
    "model = PeftModel.from_pretrained(model, adapter_path)\n",
    "\n",
    "# 3. Create the pipeline with the specified model and tokenizer\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f6af1f-e4fe-4549-81dd-fba20cfabb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in [\"ai\"]:\n",
    "    csv_file = os.path.join(\"..\", \"data\", f\"{topic}_articles.csv\")\n",
    "    df = pd.read_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab10fda-80b8-4141-8b54-2726d3f3b9e8",
   "metadata": {},
   "source": [
    "## Finetuned LlaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6090918-ab75-4c84-a99d-50d1bc339d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda:0\n",
      "The model 'PeftModelForSequenceClassification' is not supported for text-classification. Supported models are ['AlbertForSequenceClassification', 'BartForSequenceClassification', 'BertForSequenceClassification', 'BigBirdForSequenceClassification', 'BigBirdPegasusForSequenceClassification', 'BioGptForSequenceClassification', 'BloomForSequenceClassification', 'CamembertForSequenceClassification', 'CanineForSequenceClassification', 'LlamaForSequenceClassification', 'ConvBertForSequenceClassification', 'CTRLForSequenceClassification', 'Data2VecTextForSequenceClassification', 'DebertaForSequenceClassification', 'DebertaV2ForSequenceClassification', 'DiffLlamaForSequenceClassification', 'DistilBertForSequenceClassification', 'ElectraForSequenceClassification', 'ErnieForSequenceClassification', 'ErnieMForSequenceClassification', 'EsmForSequenceClassification', 'FalconForSequenceClassification', 'FlaubertForSequenceClassification', 'FNetForSequenceClassification', 'FunnelForSequenceClassification', 'GemmaForSequenceClassification', 'Gemma2ForSequenceClassification', 'GlmForSequenceClassification', 'GPT2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPTBigCodeForSequenceClassification', 'GPTNeoForSequenceClassification', 'GPTNeoXForSequenceClassification', 'GPTJForSequenceClassification', 'HeliumForSequenceClassification', 'IBertForSequenceClassification', 'JambaForSequenceClassification', 'JetMoeForSequenceClassification', 'LayoutLMForSequenceClassification', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv3ForSequenceClassification', 'LEDForSequenceClassification', 'LiltForSequenceClassification', 'LlamaForSequenceClassification', 'LongformerForSequenceClassification', 'LukeForSequenceClassification', 'MarkupLMForSequenceClassification', 'MBartForSequenceClassification', 'MegaForSequenceClassification', 'MegatronBertForSequenceClassification', 'MistralForSequenceClassification', 'MixtralForSequenceClassification', 'MobileBertForSequenceClassification', 'ModernBertForSequenceClassification', 'MPNetForSequenceClassification', 'MptForSequenceClassification', 'MraForSequenceClassification', 'MT5ForSequenceClassification', 'MvpForSequenceClassification', 'NemotronForSequenceClassification', 'NezhaForSequenceClassification', 'NystromformerForSequenceClassification', 'OpenLlamaForSequenceClassification', 'OpenAIGPTForSequenceClassification', 'OPTForSequenceClassification', 'PerceiverForSequenceClassification', 'PersimmonForSequenceClassification', 'PhiForSequenceClassification', 'Phi3ForSequenceClassification', 'PhimoeForSequenceClassification', 'PLBartForSequenceClassification', 'QDQBertForSequenceClassification', 'Qwen2ForSequenceClassification', 'Qwen2MoeForSequenceClassification', 'ReformerForSequenceClassification', 'RemBertForSequenceClassification', 'RobertaForSequenceClassification', 'RobertaPreLayerNormForSequenceClassification', 'RoCBertForSequenceClassification', 'RoFormerForSequenceClassification', 'SqueezeBertForSequenceClassification', 'StableLmForSequenceClassification', 'Starcoder2ForSequenceClassification', 'T5ForSequenceClassification', 'TapasForSequenceClassification', 'TransfoXLForSequenceClassification', 'UMT5ForSequenceClassification', 'XLMForSequenceClassification', 'XLMRobertaForSequenceClassification', 'XLMRobertaXLForSequenceClassification', 'XLNetForSequenceClassification', 'XmodForSequenceClassification', 'YosoForSequenceClassification', 'ZambaForSequenceClassification', 'Zamba2ForSequenceClassification'].\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the *base* LLaMA model for sequence classification\n",
    "base_model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer_name = \"tokenizer\"\n",
    "access_token = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_name,\n",
    "    token=access_token,\n",
    "    num_labels=5,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# Set pad token\n",
    "tokenizer.add_special_tokens({\"pad_token\":\"[PAD]\"})\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 2. Load the LoRA adapter on top of the base model\n",
    "adapter_path = \"best_model/best_model_logits_e6\"\n",
    "model = PeftModel.from_pretrained(model, adapter_path)\n",
    "\n",
    "# 3. Create the pipeline with the specified model and tokenizer\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbfd9a96-048e-4124-9ae4-60e4c1c7adb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and create dataloaders\n",
    "train_dataloader, val_dataloader = load_datasets(\n",
    "    \"../data/topics_10k_deepseek_logits.json\", None, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9bdd8ef-9d04-4ecc-ad9b-3cc66c9eabc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [02:43<00:00,  1.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8220893393084406, 0.724937343358396)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, val_dataloader, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "304b96a3-b989-4915-a3d5-5277a08b55ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [2:33:08<00:00, 3062.78s/it]  \n"
     ]
    }
   ],
   "source": [
    "for topic in tqdm([\"ai\", \"climate-change\", \"israeli-palestinian-conflict\"]):\n",
    "    csv_file = os.path.join(\"..\", \"data\", f\"{topic}_articles_llama.csv\")\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(\"Not found\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(csv_file, encoding=\"utf-8\")\n",
    "    labels = pipeline((df[\"title\"] + \" \" + df[\"article_text\"]).to_list())\n",
    "    for label in labels:\n",
    "        label[\"label\"] = label[\"label\"][-1]\n",
    "\n",
    "    df_labels = pd.DataFrame(labels)\n",
    "    df_combine = pd.concat([df.reset_index(drop=True), df_labels], axis=1)\n",
    "    df_combine = df_combine.rename(columns={\"label\": \"llama_distill_bias\"})\n",
    "\n",
    "    output_file = os.path.join(\"..\", \"data\", f\"{topic}_articles_llama_distill.csv\")\n",
    "    df_combine.to_csv(output_file, encoding=\"utf-8\", quoting=csv.QUOTE_NONNUMERIC, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5cf1cd-7a97-4f39-8274-103536f9acb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
